{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac554175-3d52-4794-9125-147882ba67b2",
   "metadata": {},
   "source": [
    "## Set Lab Black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aeffa42-6ecc-4098-9ab0-8b2dff40f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c82629-c3a9-4b51-bc46-0d394792bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38382786-4202-4b71-bbc5-ad827a6e6428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent, DRLEnsembleAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "from pprint import pprint\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a09b06-be83-439d-9bda-3793b170fc5c",
   "metadata": {},
   "source": [
    "## Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad1d87b-06c5-4289-a4f9-041721144d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "\n",
    "check_and_make_directories(\n",
    "    [DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4625f2-7ff7-49f6-aa39-a8e5bd144df1",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae0de6db-6e50-4ed1-8372-efe73c8e3ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AXP', 'AMGN', 'AAPL', 'BA', 'CAT', 'CSCO', 'CVX', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'KO', 'JPM', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'CRM', 'VZ', 'V', 'WBA', 'WMT', 'DIS', 'DOW']\n"
     ]
    }
   ],
   "source": [
    "print(DOW_30_TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbe7413c-c606-4d75-8c65-642e0f77b680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (96942, 8)\n"
     ]
    }
   ],
   "source": [
    "TRAIN_START_DATE = \"2009-04-01\"\n",
    "TRAIN_END_DATE = \"2021-01-01\"\n",
    "TEST_START_DATE = \"2021-01-01\"\n",
    "TEST_END_DATE = \"2022-06-01\"\n",
    "\n",
    "df = YahooDownloader(\n",
    "    start_date=TRAIN_START_DATE, end_date=TEST_END_DATE, ticker_list=DOW_30_TICKER\n",
    ").fetch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dcffb4-7d39-4704-985b-1b79f5243e7d",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b83eced-0af7-4f2a-b973-694abc42e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FeatureEngineer(\n",
    "    use_technical_indicator=True,\n",
    "    tech_indicator_list=INDICATORS,\n",
    "    use_turbulence=True,\n",
    "    user_defined_feature=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62984ba5-4729-4d28-83c1-9c2715281b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "processed = fs.preprocess_data(df)\n",
    "processed = processed.copy()\n",
    "processed = processed.fillna(0)\n",
    "processed = processed.replace(np.inf, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf8be62b-5719-4dbb-964f-847ca09aa5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>3.717500</td>\n",
       "      <td>3.892857</td>\n",
       "      <td>3.710357</td>\n",
       "      <td>3.308903</td>\n",
       "      <td>589372000.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.54317</td>\n",
       "      <td>3.197019</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.308903</td>\n",
       "      <td>3.308903</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>48.779999</td>\n",
       "      <td>48.930000</td>\n",
       "      <td>47.099998</td>\n",
       "      <td>36.228390</td>\n",
       "      <td>10850100.0</td>\n",
       "      <td>AMGN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.54317</td>\n",
       "      <td>3.197019</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>36.228390</td>\n",
       "      <td>36.228390</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>13.340000</td>\n",
       "      <td>14.640000</td>\n",
       "      <td>13.080000</td>\n",
       "      <td>11.772775</td>\n",
       "      <td>27701800.0</td>\n",
       "      <td>AXP</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.54317</td>\n",
       "      <td>3.197019</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11.772775</td>\n",
       "      <td>11.772775</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>34.520000</td>\n",
       "      <td>35.599998</td>\n",
       "      <td>34.209999</td>\n",
       "      <td>26.850748</td>\n",
       "      <td>9288800.0</td>\n",
       "      <td>BA</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.54317</td>\n",
       "      <td>3.197019</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>26.850748</td>\n",
       "      <td>26.850748</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>29.520000</td>\n",
       "      <td>27.440001</td>\n",
       "      <td>19.820396</td>\n",
       "      <td>15308300.0</td>\n",
       "      <td>CAT</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.54317</td>\n",
       "      <td>3.197019</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.820396</td>\n",
       "      <td>19.820396</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date       open       high        low      close       volume   tic  \\\n",
       "0  2009-04-01   3.717500   3.892857   3.710357   3.308903  589372000.0  AAPL   \n",
       "1  2009-04-01  48.779999  48.930000  47.099998  36.228390   10850100.0  AMGN   \n",
       "2  2009-04-01  13.340000  14.640000  13.080000  11.772775   27701800.0   AXP   \n",
       "3  2009-04-01  34.520000  35.599998  34.209999  26.850748    9288800.0    BA   \n",
       "4  2009-04-01  27.500000  29.520000  27.440001  19.820396   15308300.0   CAT   \n",
       "\n",
       "   day  macd  boll_ub   boll_lb  rsi_30     cci_30  dx_30  close_30_sma  \\\n",
       "0    2   0.0  3.54317  3.197019   100.0  66.666667  100.0      3.308903   \n",
       "1    2   0.0  3.54317  3.197019   100.0  66.666667  100.0     36.228390   \n",
       "2    2   0.0  3.54317  3.197019   100.0  66.666667  100.0     11.772775   \n",
       "3    2   0.0  3.54317  3.197019   100.0  66.666667  100.0     26.850748   \n",
       "4    2   0.0  3.54317  3.197019   100.0  66.666667  100.0     19.820396   \n",
       "\n",
       "   close_60_sma  turbulence  \n",
       "0      3.308903         0.0  \n",
       "1     36.228390         0.0  \n",
       "2     11.772775         0.0  \n",
       "3     26.850748         0.0  \n",
       "4     19.820396         0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb6df3d-cc61-4022-a373-6e7e657d3e32",
   "metadata": {},
   "source": [
    "## Design Enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a65076-7588-4f4b-b6e1-b43e04dc63a1",
   "metadata": {},
   "source": [
    "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "836ee46a-639a-4ade-8864-0c4c0a35f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 29, State Space: 291\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(processed.tic.unique())\n",
    "state_space = 1 + 2 * stock_dimension + len(INDICATORS) * stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "170839aa-ff34-4503-9615-146cbff72634",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"buy_cost_pct\": 0.001,\n",
    "    \"sell_cost_pct\": 0.001,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"print_verbosity\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0af9071-c4a7-47e5-99b2-25438b9c1992",
   "metadata": {},
   "source": [
    "## Implement DRL Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b56d737-a579-4621-9e28-c4fcadb96f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_window = 63  # rebalance_window is the number of days to retrain the model\n",
    "validation_window = 63  # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
    "\n",
    "ensemble_agent = DRLEnsembleAgent(\n",
    "    df=processed,\n",
    "    train_period=(TRAIN_START_DATE, TRAIN_END_DATE),\n",
    "    val_test_period=(TEST_START_DATE, TEST_END_DATE),\n",
    "    rebalance_window=rebalance_window,\n",
    "    validation_window=validation_window,\n",
    "    **env_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d13f149e-c9c0-4bad-80f9-4951c49def29",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2C_model_kwargs = {\"n_steps\": 5, \"ent_coef\": 0.005, \"learning_rate\": 0.0007}\n",
    "\n",
    "PPO_model_kwargs = {\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"n_steps\": 2048,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "\n",
    "DDPG_model_kwargs = {\n",
    "    # \"action_noise\":\"ornstein_uhlenbeck\",\n",
    "    \"buffer_size\": 10_000,\n",
    "    \"learning_rate\": 0.0005,\n",
    "    \"batch_size\": 64,\n",
    "}\n",
    "\n",
    "timesteps_dict = {\"a2c\": 10_000, \"ppo\": 10_000, \"ddpg\": 10_000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505bf593-bf5b-4320-b63a-fdf5552b56bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Start Ensemble Strategy============\n",
      "============================================\n",
      "turbulence_threshold:  203.40334064436135\n",
      "======Model training from:  2009-04-01 to  2021-01-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_126_2\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0.159      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -30.9      |\n",
      "|    reward             | -0.6531783 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.68       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -67.1     |\n",
      "|    reward             | 1.6051285 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.1       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -178     |\n",
      "|    reward             | 5.314397 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 27.9     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 57         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 32.9       |\n",
      "|    reward             | 0.02158287 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.84       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 58         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 23.1       |\n",
      "|    reward             | 0.21752164 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 8.28       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 50         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -24.8      |\n",
      "|    reward             | 0.27102497 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.405      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 58         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0.195      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -7.22      |\n",
      "|    reward             | -0.7169484 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.0861     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 66         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -0.119     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 13.4       |\n",
      "|    reward             | -0.5028562 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.379      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 60       |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 74       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -3.46    |\n",
      "|    reward             | 2.155154 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.41     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 82        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 16.3      |\n",
      "|    reward             | 1.2111665 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.345     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 90         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -133       |\n",
      "|    reward             | 0.99003005 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 12.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 99         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -3.26      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 212        |\n",
      "|    reward             | 0.15056947 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 35.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 107        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | 5.36e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -33.9      |\n",
      "|    reward             | -0.6326416 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 1.84       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 60          |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 115         |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | 106         |\n",
      "|    reward             | -0.31631353 |\n",
      "|    std                | 0.996       |\n",
      "|    value_loss         | 11.2        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 58.7      |\n",
      "|    reward             | 2.4929237 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 4.74      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 60          |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 131         |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 26.5        |\n",
      "|    reward             | -0.75668687 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 0.628       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 139       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 73.6      |\n",
      "|    reward             | 2.9364307 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 9.52      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 147       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -40.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -9.94     |\n",
      "|    reward             | 0.2028167 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 0.267     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 61        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 155       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -34.5     |\n",
      "|    reward             | 1.1890159 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 2.6       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 61          |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 163         |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41         |\n",
      "|    explained_variance | -0.0981     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 11.4        |\n",
      "|    reward             | -0.36665457 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 0.115       |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2021-01-04 to  2021-04-06\n",
      "A2C Sharpe Ratio:  0.11113815095017292\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_126_2\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 67        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 30        |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 1.1699991 |\n",
      "----------------------------------\n",
      "day: 2959, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3291255.77\n",
      "total_reward: 2291255.77\n",
      "total_cost: 379677.18\n",
      "total_trades: 82898\n",
      "Sharpe: 0.695\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 66          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014429614 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0227     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.33        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    reward               | 0.3011907   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 8.68        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 65          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017291173 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.00537    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 22          |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    reward               | -0.13909167 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 41.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 65          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 124         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015763713 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0059     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.82        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    reward               | -2.236198   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 22.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 65          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 155         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014714489 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.021       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.99        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    reward               | 0.35188717  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 14.5        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2021-01-04 to  2021-04-06\n",
      "PPO Sharpe Ratio:  0.3809198533708214\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_126_2\n",
      "day: 2959, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5607515.06\n",
      "total_reward: 4607515.06\n",
      "total_cost: 999.00\n",
      "total_trades: 44344\n",
      "Sharpe: 0.919\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 49        |\n",
      "|    time_elapsed    | 240       |\n",
      "|    total_timesteps | 11840     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 9.74      |\n",
      "|    critic_loss     | 793       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 8880      |\n",
      "|    reward          | 2.5795114 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2021-01-04 to  2021-04-06\n",
      "======Best Model Retraining from:  2009-04-01 to  2021-04-06\n",
      "======Trading from:  2021-04-06 to  2021-07-06\n",
      "============================================\n",
      "turbulence_threshold:  203.40334064436135\n",
      "======Model training from:  2009-04-01 to  2021-04-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_189_2\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0.038      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -4.49      |\n",
      "|    reward             | 0.10492845 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.4        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 60       |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 0.557    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -67.7    |\n",
      "|    reward             | 2.808757 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 3.02     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -0.0583   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -218      |\n",
      "|    reward             | 1.5486574 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 36        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -69.6      |\n",
      "|    reward             | 0.16779776 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.49       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 26.5      |\n",
      "|    reward             | 1.0286344 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.58      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -0.000612 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -131      |\n",
      "|    reward             | -8.07512  |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 120       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 57         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -65.5      |\n",
      "|    reward             | -3.4749165 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.63       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 65         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -9.12      |\n",
      "|    reward             | -1.0475771 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.05       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 73        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 305       |\n",
      "|    reward             | 1.4635667 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 63.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 82         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -67.8      |\n",
      "|    reward             | 0.80640304 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.49       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 90        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -203      |\n",
      "|    reward             | 3.9328365 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 25.4      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 60       |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 98       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -6.79    |\n",
      "|    reward             | 5.343898 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 56.1     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 106        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 20         |\n",
      "|    reward             | -0.7027108 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.256      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 114       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -40.2     |\n",
      "|    reward             | 1.0399771 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.51      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -110      |\n",
      "|    reward             | -2.795338 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.86      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 131       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -84.4     |\n",
      "|    reward             | 1.0835297 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 7.98      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 139       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -672      |\n",
      "|    reward             | 3.8709073 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 317       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 147        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -116       |\n",
      "|    reward             | -0.8882867 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 20.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 155        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -16.6      |\n",
      "|    reward             | -0.8122391 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.225      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 163        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 63.5       |\n",
      "|    reward             | -1.4490805 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.52       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-04-06 to  2021-07-06\n",
      "A2C Sharpe Ratio:  0.09856452567781043\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_189_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 66        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 30        |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 1.4627244 |\n",
      "----------------------------------\n",
      "day: 3022, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3728302.31\n",
      "total_reward: 2728302.31\n",
      "total_cost: 391234.03\n",
      "total_trades: 84861\n",
      "Sharpe: 0.733\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 65          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012413524 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0106     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.35        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    reward               | 1.1843343   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 13.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 64          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 94          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011419959 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.00508    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.2        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    reward               | 0.37658876  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 54.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 64          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 126         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017594138 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0058     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 64.5        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    reward               | 0.18645272  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 100         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 64          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016422499 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | -0.0164     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.29        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    reward               | 1.2514386   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 12.2        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2021-04-06 to  2021-07-06\n",
      "PPO Sharpe Ratio:  0.14711820153418137\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_189_1\n",
      "day: 3022, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5758109.35\n",
      "total_reward: 4758109.35\n",
      "total_cost: 1240.11\n",
      "total_trades: 50401\n",
      "Sharpe: 0.883\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 48       |\n",
      "|    time_elapsed    | 247      |\n",
      "|    total_timesteps | 12092    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.8    |\n",
      "|    critic_loss     | 184      |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 9069     |\n",
      "|    reward          | 4.6471   |\n",
      "---------------------------------\n",
      "======DDPG Validation from:  2021-04-06 to  2021-07-06\n",
      "======Best Model Retraining from:  2009-04-01 to  2021-07-06\n",
      "======Trading from:  2021-07-06 to  2021-10-04\n",
      "============================================\n",
      "turbulence_threshold:  203.40334064436135\n",
      "======Model training from:  2009-04-01 to  2021-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_252_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -0.155     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -64.7      |\n",
      "|    reward             | 0.59833056 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 7.06       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0.287     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 40        |\n",
      "|    reward             | 2.2766483 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.61      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | -0.0741  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -161     |\n",
      "|    reward             | 4.465236 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 21.7     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -0.174     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -28.2      |\n",
      "|    reward             | 0.35497072 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.15       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.0369     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -17.9      |\n",
      "|    reward             | -1.0056977 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.82       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -0.212    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 497       |\n",
      "|    reward             | 0.6066028 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 170       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 58         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -144       |\n",
      "|    reward             | 0.96010596 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 12.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 59          |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 67          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0.2         |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -126        |\n",
      "|    reward             | -0.14985979 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 8.95        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -0.111    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 97.4      |\n",
      "|    reward             | 0.9934799 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 10.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 83         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0.191      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -48.1      |\n",
      "|    reward             | -1.1619931 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.87       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 92        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -544      |\n",
      "|    reward             | 1.1240593 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 212       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 100        |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -293       |\n",
      "|    reward             | -3.7690926 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 104        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 109        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 72         |\n",
      "|    reward             | -1.1750249 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.78       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 117       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -0.0358   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -87.7     |\n",
      "|    reward             | 2.3798158 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.82      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 125        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 132        |\n",
      "|    reward             | 0.89763904 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 11.4       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 134      |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -104     |\n",
      "|    reward             | 1.68023  |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 18.1     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 59          |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 142         |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | 30.3        |\n",
      "|    reward             | -0.68557686 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.65        |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 59            |\n",
      "|    iterations         | 1800          |\n",
      "|    time_elapsed       | 150           |\n",
      "|    total_timesteps    | 9000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -41.3         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1799          |\n",
      "|    policy_loss        | -19.4         |\n",
      "|    reward             | -0.0019135929 |\n",
      "|    std                | 1.01          |\n",
      "|    value_loss         | 8.52          |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 158       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -1.79e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 21.6      |\n",
      "|    reward             | 1.3281163 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.487     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 167      |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | -0.0185  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -11.1    |\n",
      "|    reward             | 0.508374 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 0.15     |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2021-07-06 to  2021-10-04\n",
      "A2C Sharpe Ratio:  -0.10026619622423173\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_252_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 63        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 32        |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 1.3299567 |\n",
      "----------------------------------\n",
      "day: 3085, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4255108.81\n",
      "total_reward: 3255108.81\n",
      "total_cost: 413278.99\n",
      "total_trades: 86574\n",
      "Sharpe: 0.798\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 63         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 64         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01410481 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.2      |\n",
      "|    explained_variance   | -0.00681   |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 4.7        |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0254    |\n",
      "|    reward               | 0.9483288  |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 10.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 63          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014580612 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.0015      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 25          |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    reward               | 0.11594875  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 54.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 63          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 129         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015266715 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.011      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 21.5        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    reward               | 0.17494681  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 66.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 63         |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 161        |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01743782 |\n",
      "|    clip_fraction        | 0.155      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.3      |\n",
      "|    explained_variance   | -0.0324    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 6.35       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.018     |\n",
      "|    reward               | 1.2833185  |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 15.2       |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2021-07-06 to  2021-10-04\n",
      "PPO Sharpe Ratio:  -0.13692758622741222\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_252_1\n",
      "day: 3085, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5614048.57\n",
      "total_reward: 4614048.57\n",
      "total_cost: 999.00\n",
      "total_trades: 30815\n",
      "Sharpe: 0.789\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 49        |\n",
      "|    time_elapsed    | 251       |\n",
      "|    total_timesteps | 12344     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -110      |\n",
      "|    critic_loss     | 1.23e+03  |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 9258      |\n",
      "|    reward          | 3.0485282 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2021-07-06 to  2021-10-04\n",
      "======Best Model Retraining from:  2009-04-01 to  2021-10-04\n",
      "======Trading from:  2021-10-04 to  2022-01-03\n",
      "============================================\n",
      "turbulence_threshold:  203.40334064436135\n",
      "======Model training from:  2009-04-01 to  2021-10-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_315_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -0.555     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 42.3       |\n",
      "|    reward             | 0.14195059 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 6.14       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -3.54     |\n",
      "|    reward             | 2.1040452 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.143     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -246      |\n",
      "|    reward             | 4.5971613 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 42.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 23.3       |\n",
      "|    reward             | -0.8637582 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 8.99       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 235        |\n",
      "|    reward             | -2.5208259 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 32.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 50         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 790        |\n",
      "|    reward             | -2.1029463 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 381        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 59         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 2.98e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -115       |\n",
      "|    reward             | -0.7247659 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 16         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -0.565    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 135       |\n",
      "|    reward             | 0.5140665 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 16.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 75         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -0.44      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -89        |\n",
      "|    reward             | 0.93410987 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 10.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 84         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -0.0925    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -29.5      |\n",
      "|    reward             | 0.63651824 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.06       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 93        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -0.214    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 243       |\n",
      "|    reward             | -1.410022 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 39.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 101      |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | -0.0593  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 29.9     |\n",
      "|    reward             | 2.950903 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.7      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 110        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -1.54      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -35.4      |\n",
      "|    reward             | -0.9595143 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.11       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 118        |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -0.469     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 32         |\n",
      "|    reward             | 0.47877657 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 1.33       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 126        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -12.2      |\n",
      "|    reward             | 0.31725505 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 0.167      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 135       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 41.7      |\n",
      "|    reward             | 1.7515512 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 4.6       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 143        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 96.1       |\n",
      "|    reward             | 0.69154525 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 8.05       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 152        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 310        |\n",
      "|    reward             | 0.53990585 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 75.6       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 59          |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 160         |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.1       |\n",
      "|    explained_variance | -0.0115     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -28.9       |\n",
      "|    reward             | -0.33484092 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.594       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 169       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | -0.035    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -116      |\n",
      "|    reward             | -2.479795 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.73      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2021-10-04 to  2022-01-03\n",
      "A2C Sharpe Ratio:  0.18204490715581065\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_315_1\n"
     ]
    }
   ],
   "source": [
    "df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
    "                                                 PPO_model_kwargs,\n",
    "                                                 DDPG_model_kwargs,\n",
    "                                                 timesteps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99883d7e-f1ec-4b4f-9a52-03ac92103d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iter</th>\n",
       "      <th>Val Start</th>\n",
       "      <th>Val End</th>\n",
       "      <th>Model Used</th>\n",
       "      <th>A2C Sharpe</th>\n",
       "      <th>PPO Sharpe</th>\n",
       "      <th>DDPG Sharpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>PPO</td>\n",
       "      <td>0.111138</td>\n",
       "      <td>0.38092</td>\n",
       "      <td>0.335368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>189</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.098565</td>\n",
       "      <td>0.147118</td>\n",
       "      <td>0.234092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.100266</td>\n",
       "      <td>-0.136928</td>\n",
       "      <td>0.03668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>315</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.182045</td>\n",
       "      <td>0.097331</td>\n",
       "      <td>0.182273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Iter   Val Start     Val End Model Used A2C Sharpe PPO Sharpe DDPG Sharpe\n",
       "0  126  2021-01-04  2021-04-06        PPO   0.111138    0.38092    0.335368\n",
       "1  189  2021-04-06  2021-07-06       DDPG   0.098565   0.147118    0.234092\n",
       "2  252  2021-07-06  2021-10-04       DDPG  -0.100266  -0.136928     0.03668\n",
       "3  315  2021-10-04  2022-01-03       DDPG   0.182045   0.097331    0.182273"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654bee63-3dda-472d-bdf4-caf674d46d99",
   "metadata": {},
   "source": [
    "## Backtest Out Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8942a82b-dbda-4742-a2b5-ef94e420e7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_trade_date = processed[\n",
    "    (processed.date > TEST_START_DATE) & (processed.date <= TEST_END_DATE)\n",
    "].date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8010cf37-2c27-41ec-a43f-812bce060988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratio:  -0.02782960361980809\n"
     ]
    }
   ],
   "source": [
    "df_trade_date = pd.DataFrame({\"datadate\": unique_trade_date})\n",
    "\n",
    "df_account_value = pd.DataFrame()\n",
    "for i in range(\n",
    "    rebalance_window + validation_window, len(unique_trade_date) + 1, rebalance_window\n",
    "):\n",
    "    temp = pd.read_csv(\"results/account_value_trade_{}_{}.csv\".format(\"ensemble\", i))\n",
    "    df_account_value = df_account_value.append(temp, ignore_index=True)\n",
    "sharpe = (\n",
    "    (252**0.5)\n",
    "    * df_account_value.account_value.pct_change(1).mean()\n",
    "    / df_account_value.account_value.pct_change(1).std()\n",
    ")\n",
    "print(\"Sharpe Ratio: \", sharpe)\n",
    "df_account_value = df_account_value.join(\n",
    "    df_trade_date[validation_window:].reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e3a5904-bcb9-44b4-8213-fc73c0ca974f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_value</th>\n",
       "      <th>date</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>datadate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-04-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000181e+06</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>2021-04-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.998427e+05</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>-0.000338</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.001747e+06</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>2021-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000244e+06</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>-0.001501</td>\n",
       "      <td>2021-04-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_value        date  daily_return    datadate\n",
       "0   1.000000e+06  2021-04-06           NaN  2021-04-06\n",
       "1   1.000181e+06  2021-04-07      0.000181  2021-04-07\n",
       "2   9.998427e+05  2021-04-08     -0.000338  2021-04-08\n",
       "3   1.001747e+06  2021-04-09      0.001905  2021-04-09\n",
       "4   1.000244e+06  2021-04-12     -0.001501  2021-04-12"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdd3a9f9-5370-4f74-a9dd-008f0e79038e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='datadate'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value.set_index(\"datadate\").account_value.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "948b4f7f-1e7b-4041-92ed-066351d8a690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "Annual return         -0.014237\n",
      "Cumulative returns    -0.014237\n",
      "Annual volatility      0.144326\n",
      "Sharpe ratio          -0.027830\n",
      "Calmar ratio          -0.106197\n",
      "Stability              0.163804\n",
      "Max drawdown          -0.134058\n",
      "Omega ratio            0.995412\n",
      "Sortino ratio         -0.039131\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.977878\n",
      "Daily value at risk   -0.018199\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d-%Hh%M\")\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ab5d27-20b0-454f-8bb1-9aedaf4a4db2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
